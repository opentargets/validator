{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Prepare\n",
    "\n",
    "## 1.1. Calculate number of cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\r\n"
     ]
    }
   ],
   "source": [
    "! nproc --all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Download schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget -qOopentargets.json https://raw.githubusercontent.com/opentargets/json_schema/master/opentargets.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Prepare test data\n",
    "\n",
    "Test data is the top 40000 rows of the latest genetics portal evidence. This number is selected because it's 1/(4\\*60)th of the total number of EPMC evidence.\n",
    "\n",
    "Hence, the number of **seconds** required for running this sample on a 16 core machine is equivalent to the number of **minutes** required for running the full EPMC on a 64 core machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gsutil cat gs://otar000-evidence_input/Genetics_portal/json/genetics-portal-evidence-2022-04-12.json.gz | gzip 2>/dev/null -d | head -n 40000 | gzip -c -9 >test.json.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Benchmark the existing approach: opentargets_validator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "CPU times: user 4.49 ms, sys: 7.35 ms, total: 11.8 ms\n",
      "Wall time: 37.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%bash\n",
    "python3 -m opentargets_validator.cli --schema file://opentargets.json test.json.gz &>result\n",
    "echo $?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Validate directly inside Spark with `asDict` and a helper function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Install fastjsonschema library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip3 -q install --upgrade fastjsonschema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.16.1'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "import fastjsonschema\n",
    "version('fastjsonschema')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Set up Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as f\n",
    "import pyspark.sql.types as t\n",
    "\n",
    "sparkConf = (\n",
    "    SparkConf()\n",
    "    .set('spark.driver.memory', '60g')\n",
    "    .set('spark.executor.memory', '60g')\n",
    "    .set('spark.driver.maxResultSize', '0')\n",
    "    .set('spark.debug.maxToStringFields', '2000')\n",
    "    .set('spark.sql.execution.arrow.maxRecordsPerBatch', '500000')\n",
    ")\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .config(conf=sparkConf)\n",
    "    .master('local[*]')\n",
    "    .appName(\"Cerberus test\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Load data into Spark\n",
    "Of course, normally this data wouldn't be loaded from JSON, but generated by the pipeline; but it doesn't matter because the resulting dataframe (which we want to validate) is the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json('test.json.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. Load schema and compile the validator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "schema = json.load(open('opentargets.json'))\n",
    "fast_validate = fastjsonschema.compile(schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5. Prepare the validation UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, struct\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "\n",
    "def remove_none_recursively(d):\n",
    "    return {\n",
    "        k: (remove_none_recursively(v) if isinstance(v, dict) else v)\n",
    "        for k, v in d.items()\n",
    "        if v\n",
    "    }\n",
    "\n",
    "\n",
    "def validate_row(row):\n",
    "    try:\n",
    "        fast_validate(remove_none_recursively(row.asDict(recursive=True)))\n",
    "    except:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "validate_row_udf = udf(validate_row, IntegerType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6. Validate and benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 874 ms, sys: 231 µs, total: 874 ms\n",
      "Wall time: 12 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(sum(is_valid)=40000)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Calculate number of valid evidences\n",
    "new_df = df.withColumn(\"is_valid\", validate_row_udf(struct([df[x] for x in df.columns])))\n",
    "new_df.select('is_valid').groupby().sum().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Alternative PySpark approach with `to_json` → `json.loads()`\n",
    "\n",
    "This is a bit redundant (serialising and then deserialising the data), but still works with approximately the same performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_row_2(j):\n",
    "    try:\n",
    "        fast_validate(json.loads(j))\n",
    "    except:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "validate_row_udf_2 = udf(validate_row_2, IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 413 ms, sys: 2.28 ms, total: 415 ms\n",
      "Wall time: 9.5 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(sum(is_valid)=40000)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "df2 = df.withColumn(\"JSON\",f.to_json(f.struct([df[x] for x in df.columns])))\n",
    "df2 = df2.withColumn('is_valid', validate_row_udf_2(df2.JSON))\n",
    "df2.select('is_valid').groupby().sum().collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
